{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1fESObdFU5Xd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from math import sqrt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"V_228.csv\",header=None)"
      ],
      "metadata": {
        "id": "UM3AxNE9VHvX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M19phoPWEmB",
        "outputId": "4ccf8b12-f0e2-4c30-895e-cfdb46c069b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12672, 228)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=df.iloc[:,:5].values"
      ],
      "metadata": {
        "id": "HLoDEekNWKol"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rbooeerj7iEb",
        "outputId": "83453f07-16fa-47d9-d7cf-76453126ea4b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12672, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(data):\n",
        "   test_size=1440\n",
        "   validation_size=1440\n",
        "   train_data=data[:-test_size - validation_size]\n",
        "\n",
        "   validation_data=data[-test_size - validation_size:-test_size]\n",
        "   test_data=data[-test_size:]\n",
        "   return train_data,validation_data,test_data"
      ],
      "metadata": {
        "id": "_NCRjQI2WXSL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_sequences(data,window_size,forecast_horizon):\n",
        "  X=[]\n",
        "  y=[]\n",
        "  for i in range(len(data)-window_size- forecast_horizon + 1):\n",
        "        _x = data[i:(i+window_size)]\n",
        "        #_y = data[i+window_size]\n",
        "        _y=data[i + window_size:i + window_size + forecast_horizon]\n",
        "        X.append(_x)\n",
        "        y.append(_y)\n",
        "  return X,y"
      ],
      "metadata": {
        "id": "QAgegxvmWZmV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_test_sequences(data, window_size, forecast_horizon):\n",
        "    X = []\n",
        "    y = []\n",
        "    step_size = window_size + forecast_horizon\n",
        "\n",
        "    for i in range(0, len(data) - window_size - forecast_horizon + 1, step_size):\n",
        "        _x = data[i:(i + window_size)]\n",
        "        _y = data[i + window_size:i + window_size + forecast_horizon]\n",
        "        X.append(_x)\n",
        "        y.append(_y)\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "PNH0JaIM_bjS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc=MinMaxScaler()\n",
        "df_scaled=sc.fit_transform(data)"
      ],
      "metadata": {
        "id": "jiPBuOe5Wcff"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scaled.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xj0RcSHWegX",
        "outputId": "11d2de32-5a79-4a23-f018-b5e281ac8d91"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12672, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length=12\n",
        "horizon=9\n",
        "batch_size=32\n",
        "input_size=1\n",
        "hidden_size=64\n",
        "output_size=1\n",
        "ff_hiddensize=64\n",
        "mask_flag=None\n",
        "attn_head=8\n",
        "start_dec_token_len=6"
      ],
      "metadata": {
        "id": "KTJcvInWWlYN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_seq_x=[]\n",
        "train_seq_y=[]\n",
        "valid_seq_x=[]\n",
        "valid_seq_y=[]\n",
        "test_seq_x=[]\n",
        "test_seq_y=[]\n",
        "for i in range(len(df_scaled[1])):\n",
        "  train_data,validation_data,test_data = train_test_split(df_scaled[:,i])\n",
        "  train_x,train_y=create_train_sequences(train_data,sequence_length,horizon)\n",
        "  train_seq_x.append(train_x)\n",
        "  train_seq_y.append(train_y)\n",
        "\n",
        "  valid_x,valid_y=create_train_sequences(validation_data,sequence_length,horizon)\n",
        "  valid_seq_x.append(valid_x)\n",
        "  valid_seq_y.append(valid_y)\n",
        "\n",
        "  test_x,test_y=create_test_sequences(test_data,sequence_length,horizon)\n",
        "  test_seq_x.append(test_x)\n",
        "  test_seq_y.append(test_y)\n"
      ],
      "metadata": {
        "id": "9zbB2tpXWnlZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=torch.tensor(train_seq_x,dtype=torch.float32)\n",
        "y_train=torch.tensor(train_seq_y,dtype=torch.float32)\n",
        "X_valid=torch.tensor(valid_seq_x,dtype=torch.float32)\n",
        "y_valid=torch.tensor(valid_seq_y,dtype=torch.float32)\n",
        "X_test=torch.tensor(test_seq_x,dtype=torch.float32)\n",
        "y_test=torch.tensor(test_seq_y,dtype=torch.float32)"
      ],
      "metadata": {
        "id": "3uSzaKpCOXmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad10463b-cd0b-436c-ebad-1e8b44168e21"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-1f87e33d132c>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
            "  X_train=torch.tensor(train_seq_x,dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=X_train.unsqueeze(-1)\n",
        "X_train=X_train.view(-1,X_train.size(2),X_train.size(3))\n",
        "\n",
        "y_train=y_train.unsqueeze(-1)\n",
        "y_train=y_train.view(-1,y_train.size(2),y_train.size(3))\n",
        "\n",
        "X_valid=X_valid.unsqueeze(-1)\n",
        "X_valid=X_valid.view(-1,X_valid.size(2),X_valid.size(3))\n",
        "\n",
        "y_valid=y_valid.unsqueeze(-1)\n",
        "y_valid=y_valid.view(-1,y_valid.size(2),y_valid.size(3))\n",
        "\n",
        "X_test=X_test.unsqueeze(-1)\n",
        "y_test=y_test.unsqueeze(-1)\n",
        "\"\"\"\n",
        "X_test=X_test.unsqueeze(-1)\n",
        "X_test=X_test.view(-1,X_test.size(2),X_test.size(3))\n",
        "\n",
        "y_test=y_test.unsqueeze(-1)\n",
        "y_test=y_test.view(-1,y_test.size(2),y_test.size(3))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "aS7NfdeAwOqZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f78696a5-d514-461c-d5aa-6fc9fc5685a1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nX_test=X_test.unsqueeze(-1)\\nX_test=X_test.view(-1,X_test.size(2),X_test.size(3))\\n\\ny_test=y_test.unsqueeze(-1)\\ny_test=y_test.view(-1,y_test.size(2),y_test.size(3))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeseriesDataset(Dataset):\n",
        "  def __init__(self,X,y):\n",
        "    self.X=X\n",
        "    self.y=y\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "  def __getitem__(self,idx):\n",
        "    return self.X[idx],self.y[idx]"
      ],
      "metadata": {
        "id": "lVjZ-zrcLewd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=TimeseriesDataset(X_train,y_train)\n",
        "valid_dataset=TimeseriesDataset(X_valid,y_valid)\n",
        "test_dataset=TimeseriesDataset(X_test,y_test)"
      ],
      "metadata": {
        "id": "mPMbcoQ1PGrm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=DataLoader(train_dataset,batch_size,drop_last=True)\n",
        "valid_loader=DataLoader(valid_dataset,batch_size,drop_last=True)\n",
        "test_loader=DataLoader(test_dataset,batch_size,drop_last=True)"
      ],
      "metadata": {
        "id": "xEsyYaYpPJWd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size):\n",
        "      super().__init__()\n",
        "      self.input_size=input_size\n",
        "      self.hidden_size=hidden_size\n",
        "      self.conv1d=nn.Conv1d(in_channels=self.input_size,out_channels=self.hidden_size,padding=1,kernel_size=3,bias=False)\n",
        "  def forward(self,x):\n",
        "      embedded_inp=self.conv1d(x.permute(0,2,1))\n",
        "      return embedded_inp.transpose(1,2)\n"
      ],
      "metadata": {
        "id": "1BdJIZImOjLl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe.require_grad = False\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float()* -(math.log(10000.0) / d_model)).exp()\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self,x):\n",
        "        return self.pe[:,x.size(1)]"
      ],
      "metadata": {
        "id": "DnnOQkO7OmtX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "  def __init__(self,attn_head,hidden_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attn_head=attn_head\n",
        "    self.hidden_size=hidden_size\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.linear=nn.Linear(hidden_size,hidden_size)\n",
        "\n",
        "    self.queries=nn.Linear(hidden_size,hidden_size)\n",
        "    self.keys=nn.Linear(hidden_size,hidden_size)\n",
        "    self.values=nn.Linear(hidden_size,hidden_size)\n",
        "\n",
        "  def forward(self, queries,keys,values,attention_mask):\n",
        "    b,l,d=queries.shape\n",
        "    b,s,d=keys.shape\n",
        "\n",
        "    #Linear projection and creation of multiple heads\n",
        "    queries=self.queries(queries).view(b,l,self.attn_head,-1)\n",
        "    keys=self.keys(keys).view(b,s,self.attn_head,-1)\n",
        "    values=self.values(values).view(b,s,self.attn_head,-1)\n",
        "\n",
        "    b,l,h,d=queries.shape\n",
        "    b,s,h,d=keys.shape\n",
        "\n",
        "    #Calculate attention score\n",
        "    attention_score=torch.einsum(\"blhd,bshd->bhls\",queries,keys)\n",
        "    if attention_mask == True:\n",
        "       mask_shape = [b,h,l,s]\n",
        "       mask=torch.triu(torch.ones(mask_shape,dtype=torch.bool),diagonal=1)\n",
        "       attention_score.masked_fill_(mask,-np.inf)\n",
        "    attention_score_softmax=self.dropout(torch.softmax(attention_score/sqrt(d),dim=-1))\n",
        "    final_value=torch.einsum(\"bhls,bshd->blhd\",attention_score_softmax,values)\n",
        "\n",
        "    weighted_attn_val=self.linear(final_value.reshape(b,l,-1))\n",
        "\n",
        "    return weighted_attn_val"
      ],
      "metadata": {
        "id": "Vc7rG2iSOqlz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,attention,hidden_size,output_size,ff_hiddensize,sequence_length):\n",
        "    super().__init__()\n",
        "    self.attention=attention\n",
        "    self.conv1=nn.Conv1d(in_channels=hidden_size,out_channels=ff_hiddensize,kernel_size=1)\n",
        "    self.conv2=nn.Conv1d(in_channels=ff_hiddensize,out_channels=hidden_size,kernel_size=1)\n",
        "    self.linear=nn.Linear(ff_hiddensize,hidden_size)\n",
        "    self.norm1=nn.LayerNorm(normalized_shape=(sequence_length,hidden_size))\n",
        "    self.activation=F.relu\n",
        "  def forward(self,x):\n",
        "    #self.norm2=nn.LayerNorm(hidden_size)\n",
        "    attention_x=self.attention(x,x,x,attention_mask=False)\n",
        "    #add and normalize\n",
        "    new_x = x + attention_x\n",
        "    res_x=x=self.norm1(new_x)\n",
        "    ##Feed forward NN:\n",
        "    out=self.conv1(res_x.permute(0,2,1))\n",
        "    out=self.activation(out)\n",
        "    out=self.conv2(out).transpose(-1, 1)\n",
        "\n",
        "    ##Add and normalize:\n",
        "    new_out=out+res_x\n",
        "    norm_out=self.norm1(new_out)\n",
        "    return norm_out"
      ],
      "metadata": {
        "id": "9ixwqm1oOtXQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,attention,hidden_size,output_size,ff_hiddensize,sequence_length):\n",
        "    super().__init__()\n",
        "    self.attention=attention\n",
        "    #print(f\"decoder output size: {output_size}\")\n",
        "    self.conv1=nn.Conv1d(in_channels=hidden_size,out_channels=ff_hiddensize,kernel_size=1)\n",
        "    self.conv2=nn.Conv1d(in_channels=ff_hiddensize,out_channels=hidden_size,kernel_size=1)\n",
        "    self.linear1=nn.Linear(hidden_size,ff_hiddensize)\n",
        "    self.linear2=nn.Linear(ff_hiddensize,hidden_size)\n",
        "    self.linear3=nn.Linear(hidden_size,output_size)\n",
        "\n",
        "    self.norm1=nn.LayerNorm(hidden_size)\n",
        "    self.norm2=nn.LayerNorm(hidden_size)\n",
        "    self.norm3=nn.LayerNorm(hidden_size)\n",
        "    self.activation=F.relu\n",
        "  def forward(self,dec_inp,enc_out):\n",
        "    #calculate self attention by passing dec_inp as Queries,keys and values\n",
        "    self_attn=self.attention(dec_inp,dec_inp,dec_inp,attention_mask=True)\n",
        "    #add residual connection and normalize\n",
        "    residual_add=self_attn+dec_inp\n",
        "    new_dec_x=self.norm1(residual_add)\n",
        "\n",
        "    # encoder-decoder attention. Pass key and value as encoder output and queries are output of 1st attention\n",
        "    enc_dec_atten=self.attention(new_dec_x,enc_out,enc_out,attention_mask=False)\n",
        "    ## add and normalize\n",
        "    new_x=enc_dec_atten+self_attn\n",
        "    norm_x=self.norm2(new_x)\n",
        "\n",
        "    #FFN\n",
        "    out=self.conv1(norm_x.permute(0,2,1))\n",
        "    out=self.activation(out)\n",
        "    out=self.conv2(out).transpose(-1, 1)\n",
        "\n",
        "    #add and normalize\n",
        "    new_x=out+norm_x\n",
        "    out=self.norm3(new_x)\n",
        "\n",
        "\n",
        "    #Linear projection\n",
        "    pred=self.linear3(out)\n",
        "\n",
        "    return pred\n"
      ],
      "metadata": {
        "id": "defFCswFOwsC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,output_size,ff_hiddensize,mask_flag,attn_head,sequence_length):\n",
        "      super().__init__()\n",
        "      self.input_size=input_size\n",
        "      self.hidden_size=hidden_size\n",
        "      self.output_size=output_size\n",
        "      self.ff_hidden_size=hidden_size\n",
        "      self.mask_flag=mask_flag\n",
        "      self.attn_head=attn_head\n",
        "      self.sequence_length=sequence_length\n",
        "      self.embedding=InputEmbedding(self.input_size,self.hidden_size)\n",
        "      self.positional_embedding=PositionalEmbedding(self.hidden_size)\n",
        "      self.encoder=Encoder( AttentionLayer(self.attn_head,hidden_size),self.hidden_size,self.output_size,self.ff_hidden_size,sequence_length)\n",
        "      self.decoder=Decoder( AttentionLayer(self.attn_head,hidden_size),self.hidden_size,self.output_size,self.ff_hidden_size,sequence_length )\n",
        "  def forward(self,x,y):\n",
        "\n",
        "      #dec inp:\n",
        "      #input is shifted right and concatenated with 1st time step embedding set as zero\n",
        "      #decoder_input = torch.cat([torch.zeros_like(y[:, :1, :]), y[:, :-1, :]], dim=1)\n",
        "      #print(f\"x shape: {x[:, -start_dec_token_len:, :].shape}, y shape: {y[:, -horizon:, :].shape}\")\n",
        "      #print(f\"x shape: {x.shape}, y shape: {y.shape}\")\n",
        "      #decoder_input = torch.cat((x[:, -start_dec_token_len:, :], torch.zeros_like(y[:, -horizon:, :])), dim=1)\n",
        "      #decoder_input = torch.cat((x[:, :,-start_dec_token_len ,:], torch.zeros_like(y[:, :,-horizon:, :])), dim=1)\n",
        "      #print(f\"decoder_input shape: {decoder_input.shape}\")\n",
        "\n",
        "      decoder_input=torch.zeros_like(y[:, -horizon:, :])\n",
        "\n",
        "      #encoder\n",
        "      inp_embed=self.embedding(x)\n",
        "      pos_embed=self.embedding(x)\n",
        "      x_embed=inp_embed + pos_embed\n",
        "      enc_out=self.encoder(x_embed)\n",
        "\n",
        "      #decoder\n",
        "      inp_embed=self.embedding(decoder_input)\n",
        "      pos_embed=self.embedding(decoder_input)\n",
        "      y_embed= inp_embed + pos_embed\n",
        "      #print(f\"y passed to decoder shape: {y.shape}\")\n",
        "      out=self.decoder(y_embed,enc_out)\n",
        "      return out\n"
      ],
      "metadata": {
        "id": "jnI2WldAO0ci"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=TransformerModel(input_size,hidden_size,output_size,ff_hiddensize,mask_flag,attn_head,sequence_length)"
      ],
      "metadata": {
        "id": "-KlFfL9wO3bN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fun=nn.MSELoss()\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=0.01)\n",
        "epochs=30"
      ],
      "metadata": {
        "id": "PlgwBTAcO5NG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "        for batch_idx, (X,y) in enumerate(train_loader):\n",
        "\n",
        "            pred=model(X,y)\n",
        "            pred=pred[:,-horizon:,:]\n",
        "            optimizer.zero_grad()\n",
        "            loss=loss_fun(pred,y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        if epoch%10==0:\n",
        "            print(f\"epoch: {epoch} train loss:{loss} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuAyRTueO7LB",
        "outputId": "6dce40fe-e4c7-4aa0-da64-b2299315d193"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 train loss:0.04169631004333496 \n",
            "epoch: 10 train loss:0.04237790405750275 \n",
            "epoch: 20 train loss:0.04239429906010628 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output=[]\n",
        "ground_truth=[]\n",
        "pred_series=[]\n",
        "truth_series=[]\n",
        "loss=[]\n",
        "pred_total=[]\n",
        "y_total=[]\n",
        "\n",
        "for i in range(X_test.size(0)):\n",
        "  curent_X_test=X_test[i,:,:,:]\n",
        "  current_y_test=y_test[i,:,:,:]\n",
        "  pred=model(curent_X_test,current_y_test)\n",
        "  pred=pred[:,-horizon:,:]\n",
        "  pred=pred.reshape(-1,1).detach().numpy()\n",
        "  current_y_test=current_y_test.reshape(-1,1).detach().numpy()\n",
        "\n",
        "  loss.append(loss_fun(torch.tensor(pred),torch.tensor(current_y_test)))\n",
        "\n"
      ],
      "metadata": {
        "id": "QE_rrOVFWXjm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KSaCeCEoTpw",
        "outputId": "d66a01f7-74dc-4b38-fafa-3475071aea53"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor(0.0753),\n",
              " tensor(0.0928),\n",
              " tensor(0.0408),\n",
              " tensor(0.0407),\n",
              " tensor(0.0368)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}