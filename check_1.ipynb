{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "import statistics \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "plot_dir = './plots/energy_relative/'\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fix_seed = 1111\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "dataset=\"traffic\" #traffic,energy\n",
    "with_knowledge=False #True,False\n",
    "positional_embedding=\"relative\" #absolute,relative\n",
    "attention=\"vanilla\" #vanilla,autoformer,auto_correlation\n",
    "\n",
    "#normalization=\"standard_scalar\"\n",
    "test_with_attention=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='traffic'\n",
    "normalize=False\n",
    "know_different_norm=False\n",
    "with_knowledge=True\n",
    "sequence_length=12\n",
    "horizon=9\n",
    "batch_size=32\n",
    "input_size=1\n",
    "hidden_size=64\n",
    "output_size=1\n",
    "ff_hiddensize=64\n",
    "mask_flag=None\n",
    "attn_head=8\n",
    "label_length=4\n",
    "test_size=1440\n",
    "validation_size=1440*2\n",
    "context_window=24\n",
    "knowledge_pred_path='/home/chatta/sarima_pemM/github/knowledge_preds/'+dataset+'/horizon_'+str(horizon)+'/t_preds.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset==\"traffic\":\n",
    "    \n",
    "    df=np.asarray(pd.read_csv(\"./dataset/V_228.csv\",header=None))\n",
    "    df_know=np.asarray(pd.read_csv(knowledge_pred_path,header=None))\n",
    "elif dataset==\"energy\":\n",
    "    \n",
    "    df=np.asarray(pd.read_csv(\"./dataset/energydata_complete.txt\",header=None))\n",
    "    df_know=np.asarray(pd.read_csv(knowledge_pred_path,header=None))\n",
    "elif dataset==\"traffic_auto\":\n",
    "    df = pd.read_csv(\"./dataset/traffic_auto/traffic.csv\",\n",
    "    header=None,\n",
    "    dtype={862: 'float64'},\n",
    "    skiprows=[0] )\n",
    "    df=df.iloc[:,1:]\n",
    "    df_know=pd.read_csv(\"./dataset/final_knowledge/final/traffic_auto/h96/t_preds.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input(data,window_size,horizon=1):\n",
    "    length=data.shape[0]\n",
    "    y = np.zeros([length-window_size+1-horizon,horizon])\n",
    "    output=np.zeros([length-window_size+1-horizon,window_size])\n",
    "    for i in range(length-window_size-horizon+1):\n",
    "        output[i:i+1,:]=data[i:i+window_size]\n",
    "        y[i,:]= data[i+window_size:i+window_size+horizon]\n",
    "    return output.reshape(output.shape[0],window_size), y\n",
    "\n",
    "def make_k_input(data,horizon):\n",
    "    length = data.shape[0]\n",
    "    output= np.zeros([length+1-horizon,horizon])\n",
    "    for i in range(length-horizon+1):\n",
    "        output[i:i+1,:]=data[i:i+horizon]\n",
    "    return output.reshape(output.shape[0],horizon)\n",
    "\n",
    "def nonov_make_input(data,window_size,horizon=1):\n",
    "    length=data.shape[0]-window_size\n",
    "    loop=length//horizon\n",
    "    extra = length%horizon\n",
    "\n",
    "    data = np.append(data,np.zeros([horizon-extra]))\n",
    "\n",
    "    if extra ==0:\n",
    "        i_val = loop\n",
    "    else:\n",
    "        i_val=loop+1\n",
    "        \n",
    "    output=np.zeros([i_val,window_size])\n",
    "    y=np.zeros([i_val,horizon])\n",
    "    for i in range(i_val):\n",
    "        output[i:i+1,:]=data[i*horizon:(i*horizon)+window_size]\n",
    "        y[i,:]= data[(i*horizon)+window_size:(i*horizon)+window_size+horizon]\n",
    "        \n",
    "    return output.reshape(output.shape[0],window_size), y\n",
    "\n",
    "def nonov_make_k_input(data,horizon):\n",
    "    length = data.shape[0]\n",
    "    loop=length//horizon\n",
    "    extra = length%horizon\n",
    "    data_app = np.repeat(data[-1],(horizon-extra))\n",
    "    data = np.append(data,data_app)    \n",
    "\n",
    "    if extra ==0:\n",
    "        i_val = loop\n",
    "    else:\n",
    "        i_val=loop+1\n",
    "    output=np.zeros([i_val,horizon])\n",
    "    for i in range(i_val):\n",
    "        output[i:i+1,:]=data[(i*horizon):(i*horizon)+horizon]\n",
    "    return output.reshape(output.shape[0],horizon)\n",
    "\n",
    "def normalize(x):\n",
    "    min_in=np.min(x,axis=1).reshape(x.shape[0],1)\n",
    "    max_in=np.max(x,axis=1).reshape(x.shape[0],1) \n",
    "    denom=max_in-min_in\n",
    "    a = np.where(denom == 0)[0]\n",
    "    denom[a] = max_in[a] \n",
    "    a = np.where(denom == 0)[0]\n",
    "    if a.size>0:\n",
    "        denom[a]=1\n",
    "    x=(x-min_in)/(denom)\n",
    "    return x,min_in,denom\n",
    "\n",
    "def theta_normalize(theta,min_in,denom):\n",
    "  \n",
    "    \n",
    "    theta=(theta-min_in)/denom\n",
    "    return theta\n",
    "\n",
    "\n",
    "\n",
    "def metrics(pred,gt):\n",
    "    l = pred.shape[1]\n",
    "#     print(l)\n",
    "    err_mse = np.zeros((l))\n",
    "    err_mae = np.zeros((l))\n",
    "\n",
    "    for i in range(l):\n",
    "        err_mse[i] = mse(pred[:,i],gt[:,i])\n",
    "        err_mae[i] = mae(pred[:,i],gt[:,i])\n",
    "        \n",
    "    return np.sqrt(np.mean(err_mse)),np.mean(err_mae)\n",
    "\n",
    "def window_normalize(data_x,data_y):\n",
    "    \n",
    "    \n",
    "    min_in = np.min(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    max_in = np.max(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    denom = (max_in-min_in)\n",
    "    a = np.where(denom == 0)[0]\n",
    "    denom[a] = max_in[a] \n",
    "    a = np.where(denom == 0)[0]\n",
    "    if a.size >0:\n",
    "        denom[a]=1\n",
    "    out = (data_x-min_in)/denom\n",
    "    out=out.reshape(out.shape[0],out.shape[1],1)\n",
    "    out_y=(data_y-min_in)/denom\n",
    "    \n",
    "    return out, out_y, denom, min_in\n",
    "\n",
    "def window_normalize_with_expert(data_x,data_y,expert):\n",
    "    \n",
    "    min_in = np.min(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    max_in = np.max(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    denom = (max_in-min_in)\n",
    "    a = np.where(denom == 0)[0]\n",
    "    denom[a] = max_in[a] \n",
    "    a = np.where(denom == 0)[0]\n",
    "    if a.size >0:\n",
    "        denom[a]=1\n",
    "    out = (data_x-min_in)/denom\n",
    "    expert_normd=(expert-min_in)/denom\n",
    "    out=np.append(out,expert_normd,axis=1)\n",
    "    out=out.reshape(out.shape[0],out.shape[1],1)\n",
    "    out_y=(data_y-min_in)/denom\n",
    "    \n",
    "    return out,out_y,expert_normd,denom,min_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_test_split(data,validation_size,test_size,sequence_length):\n",
    "   \n",
    "\n",
    "#    train_data=data[:-test_size - validation_size]\n",
    "   \n",
    "#    validation_data=data[-test_size - validation_size-sequence_length:-test_size]\n",
    "#    test_data=data[-test_size-sequence_length:]\n",
    "#    return train_data,validation_data,test_data\n",
    "\n",
    "# def train_test_split_know(data):\n",
    "   \n",
    "#    test_size=1440\n",
    "#    validation_size=1440\n",
    "#    train_data=data[:-test_size - validation_size]\n",
    "\n",
    "#    validation_data=data[-test_size - validation_size:-test_size]\n",
    "#    test_data=data[-test_size:]\n",
    "#    return train_data,validation_data,test_data   \n",
    "\n",
    "# #Create past window and horizon sequences\n",
    "# def create_train_sequences(data,window_size,forecast_horizon):\n",
    "#   #X=[]\n",
    "#   #y=[]\n",
    "  \n",
    "#   X_shape=[(len(data)-window_size-forecast_horizon + 1),window_size]\n",
    "#   y_shape=[(len(data)-window_size-forecast_horizon + 1),forecast_horizon]\n",
    "#   X=np.zeros(X_shape)\n",
    "#   y=np.zeros(y_shape)\n",
    "#   for i in range(len(data)-window_size- forecast_horizon + 1):\n",
    "#         _x = data[i:(i+window_size)]\n",
    "#         #_y = data[i+window_size]\n",
    "#         _y=data[i + window_size:i + window_size + forecast_horizon]\n",
    "#         X[i,:]=_x\n",
    "#         y[i,:]=_y\n",
    "  \n",
    "#   return X,y\n",
    "\n",
    "# def create_test_seq(data, window_size, horizon):\n",
    "#     length=data.shape[0]-window_size\n",
    "#     loop=length//horizon\n",
    "#     extra = length%horizon\n",
    "    \n",
    "#     if extra ==0:\n",
    "#         i_val = loop\n",
    "#     else:\n",
    "#         i_val=loop+1\n",
    "    \n",
    "#     data = np.append(data,np.zeros([horizon-extra]))    \n",
    "#     output=np.zeros([i_val,window_size])\n",
    "#     y=np.zeros([i_val,horizon])\n",
    "\n",
    "#     for i in range(i_val):\n",
    "#         output[i:i+1,:]=data[i*horizon:(i*horizon)+window_size]\n",
    "#         y[i,:]= data[(i*horizon)+window_size:(i*horizon)+window_size+horizon]\n",
    "        \n",
    "    \n",
    "#     return output.reshape(output.shape[0],window_size), y\n",
    "\n",
    "# #Create horizon sequences of knowledge prediction\n",
    "# def create_train_knowledge_seq(data,window_size,horizon):\n",
    "    \n",
    "#     y_shape=[(len(data)-horizon + 1),horizon]\n",
    "#     y_know=np.zeros(y_shape)\n",
    "      \n",
    "#     for i in range(len(data)-window_size- horizon + 1):\n",
    "        \n",
    "#         _y_know=data[i:i+horizon]\n",
    "#         y_know[i,:]=_y_know\n",
    "\n",
    "#     return y_know\n",
    "\n",
    "# def create_test_k_seq(data,horizon):\n",
    "    \n",
    "#     length = data.shape[0]\n",
    "#     loop=length//horizon\n",
    "#     extra = length%horizon\n",
    "#     if extra ==0:\n",
    "#         i_val = loop\n",
    "#     else:\n",
    "#         i_val=loop+1\n",
    "\n",
    "#     data_app = np.repeat(data[-1],(horizon-extra))\n",
    "#     data = np.append(data,data_app)  \n",
    "#     output=np.zeros([i_val,horizon])\n",
    "           \n",
    "#     for i in range(i_val):\n",
    "#         output[i:i+1,:]=data[(i*horizon):(i*horizon)+horizon]\n",
    "\n",
    "#     return output.reshape(output.shape[0],horizon)\n",
    "\n",
    "def create_attention_seq(data,window_size,forecast_horizon,context_window):\n",
    "  X_attention=np.zeros([len(data)-window_size- forecast_horizon + 1 , context_window])\n",
    "  #print(X_attention.shape,len(data)-window_size- forecast_horizon + 1)\n",
    "  end_index=0   \n",
    "  for i in range(len(data)-window_size- forecast_horizon + 1):     \n",
    "      if end_index < context_window-1:\n",
    "         _x=data[0:window_size+i]  \n",
    "         end_index=window_size+i\n",
    "         \"\"\"\n",
    "         zeros_to_add=[0]*(context_window - len(_x))\n",
    "         _x = pd.concat([ pd.Series(zeros_to_add),pd.Series(_x)],ignore_index=True)\n",
    "         _x=_x.values  \n",
    "         \"\"\"         \n",
    "         first_data_value = data[0]\n",
    "         fill_values = [first_data_value] * (context_window - len(_x))\n",
    "         _x = pd.concat([ pd.Series(fill_values),pd.Series(_x)],ignore_index=True)\n",
    "         _x=_x.values\n",
    "         \n",
    "      else:     \n",
    "         _x=data[window_size+i-context_window:window_size+i]         \n",
    "      X_attention[i,:]=_x \n",
    "  return X_attention\n",
    "\n",
    "def create_attention_test_sequences(data, window_size, forecast_horizon,context_window):\n",
    "    X_attention=np.zeros((len(data),context_window))   \n",
    "    for i in range(len(data)):       \n",
    "        if(i<=int(context_window/sequence_length - 2)):        \n",
    "            subset_x=data[0:i+1] \n",
    "            \"\"\"\n",
    "            zeros_to_add=[0]*(context_window - len(subset_x.reshape(-1)) )\n",
    "            _x = pd.concat([ pd.Series(zeros_to_add),pd.Series(subset_x.reshape(-1))],ignore_index=True) \n",
    "            _x=_x.values            \n",
    "            \"\"\"\n",
    "            first_data_value = data[0][0] \n",
    "            #print(f\"first_data_value:{first_data_value}\")\n",
    "            fill_values = [first_data_value] * (context_window - len(subset_x.reshape(-1)))\n",
    "            _x = pd.concat([ pd.Series(fill_values),pd.Series(subset_x.reshape(-1))],ignore_index=True)\n",
    "            _x = _x.values                 \n",
    "        else:     \n",
    "            start_index=int(i-((context_window/sequence_length)-1))        \n",
    "            _x=data[start_index:i+1].reshape(-1) \n",
    "        X_attention[i]=_x                \n",
    "    return X_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size//horizon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers=[]\n",
    "know_scalers=[]\n",
    " \n",
    "train_seq_x=np.zeros([df.shape[1], (len(df)-validation_size-sequence_length-horizon+1) , sequence_length ])\n",
    "train_seq_y=np.zeros([df.shape[1], (len(df)-validation_size-sequence_length-horizon+1) , horizon])\n",
    "valid_seq_x=np.zeros([df.shape[1], validation_size-test_size-horizon+1  , sequence_length ]) \n",
    "valid_seq_y=np.zeros([df.shape[1], validation_size-test_size-horizon+1 , horizon ])\n",
    "test_seq_x=np.zeros([df.shape[1], test_size // horizon, sequence_length ])\n",
    "test_seq_y=np.zeros([df.shape[1],  test_size //horizon, horizon])\n",
    "\n",
    "train_attention_x=np.zeros([df.shape[1], ((len(df)-validation_size-sequence_length-horizon+1)), context_window])\n",
    "valid_attention_x=np.zeros([df.shape[1],  validation_size-test_size-horizon+1, context_window])\n",
    "test_attention_x=np.zeros([df.shape[1], test_size // horizon  , context_window])\n",
    "\n",
    "if with_knowledge==True:\n",
    "    train_know_seq_y=np.zeros([df_know.shape[1], (len(df_know)-validation_size-horizon+1) , horizon])\n",
    "    valid_know_seq_y=np.zeros([df_know.shape[1], validation_size-test_size-horizon+1 , horizon ])\n",
    "    test_know_seq_y=np.zeros([df_know.shape[1],  test_size // horizon, horizon])\n",
    "\n",
    "for i in range(df.shape[1]):\n",
    "    current_row=df[:,i]\n",
    "    train_data = current_row[:-validation_size]\n",
    "    val_data = current_row[-(validation_size+sequence_length):-test_size]\n",
    "    test_data = current_row[-(test_size+sequence_length):]\n",
    "    train_sequence = make_input(train_data, sequence_length,horizon)\n",
    "    val_sequence = make_input(val_data,sequence_length,horizon)\n",
    "    test_sequence = nonov_make_input(test_data,sequence_length,horizon)\n",
    "    \n",
    "    know_data=df_know[:,i]\n",
    "    train_know_data=know_data[:-validation_size]\n",
    "    val_know_data=know_data[-validation_size:-test_size]\n",
    "    test_know_data=know_data[-test_size:]\n",
    "    train_know_seq=make_k_input(train_know_data,horizon)\n",
    "    val_know_seq=make_k_input(val_know_data,horizon)\n",
    "    test_know_seq=nonov_make_k_input(test_know_data,horizon)\n",
    "    \n",
    "    if normalize==True:\n",
    "            \n",
    "        \n",
    "        temp_train_seq_x[i,:,:]=train_sequence[0]\n",
    "        temp_train_seq_y[i,:,:]=train_sequence[1]\n",
    "    \n",
    "        temp_val_seq_x[i,:,:]=val_sequence[0]\n",
    "        temp_val_seq_y[i,:,:]=val_sequence[1]\n",
    "    \n",
    "        temp_test_seq_x[i,:,:]=test_sequence[0]\n",
    "        temp_test_seq_y[i,:,:]=test_sequence[1]\n",
    "        if know_different_norm==False:\n",
    "            temp_train_seq_x,temp_train_seq_y,temp_train_know_seq,_,_=window_normalize_with_expert(temp_train_seq_x,temp_train_seq_y,train_know_seq)\n",
    "            temp_val_seq_x,temp_val_seq_y,temp_val_know_seq,_,_=window_normalize_with_expert(temp_val_seq_x,temp_val_seq_y,val_know_seq)\n",
    "            temp_test_seq_x,temp_test_seq_y,temp_test_know_seq,_,_=window_normalize_with_expert(temp_test_seq_x,temp_test_seq_y,test_know_seq)\n",
    "        else:\n",
    "            temp_train_seq_x,temp_train_seq_y,_,_=window_normalize(temp_train_seq_x,temp_train_seq_y)\n",
    "            temp_val_seq_x,temp_val_seq_y,_,_=window_normalize(temp_val_seq_x,temp_val_seq_y)\n",
    "            temp_test_seq_x,temp_test_seq_y,_,_=window_normalize(temp_test_seq_x,temp_test_seq_y)\n",
    "            \n",
    "            temp_train_know_seq=normalize(train_know_seq)\n",
    "            temp_val_know_seq=normalize(val_know_seq)\n",
    "            temp_test_know_seq=normalize(test_know_seq)\n",
    "        \n",
    "        \n",
    "        train_seq_x[i,:,:]=temp_train_seq_x\n",
    "        train_seq_y[i,:,:]=temp_train_seq_y\n",
    "        valid_seq_x[i,:,:]=temp_val_seq_x\n",
    "        valid_seq_y[i,:,:]=temp_val_seq_y\n",
    "        test_seq_x[i,:,:]=temp_test_seq_x\n",
    "        test_seq_y[i,:,:]=temp_test_seq_y\n",
    "        \n",
    "        \n",
    "        train_know_seq_y[i,:,:]= temp_train_know_seq\n",
    "        valid_know_seq_y[i,:,:]= temp_val_know_seq\n",
    "        test_know_seq_y[i,:,:]=temp_test_know_seq\n",
    "    \n",
    "    else:\n",
    "        train_know_seq_y[i,:,:]= train_know_seq\n",
    "        valid_know_seq_y[i,:,:]= val_know_seq\n",
    "        test_know_seq_y[i,:,:]=test_know_seq\n",
    "    \n",
    "    \n",
    "        train_seq_x[i,:,:]=train_sequence[0]\n",
    "        train_seq_y[i,:,:]=train_sequence[1]\n",
    "    \n",
    "        valid_seq_x[i,:,:]=val_sequence[0]\n",
    "        valid_seq_y[i,:,:]=val_sequence[1]\n",
    "    \n",
    "        test_seq_x[i,:,:]=test_sequence[0]\n",
    "        test_seq_y[i,:,:]=test_sequence[1]\n",
    "        \n",
    "        train_know_seq_y[i,:,:]= train_know_seq\n",
    "        valid_know_seq_y[i,:,:]= val_know_seq\n",
    "        test_know_seq_y[i,:,:]=test_know_seq\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "#     train_know_data,validation_know_data,test_know_data = train_test_split_know(df_know.iloc[:,i])\n",
    "    \n",
    "#     scaler=StandardScaler()\n",
    "#     train_data_scaled=scaler.fit_transform(train_data.values.reshape(-1, 1))\n",
    "#     scalers.append(scaler)\n",
    "        \n",
    "#     validation_data_scaled = scaler.transform(validation_data.values.reshape(-1, 1))      \n",
    "#     test_data_scaled = scaler.transform(test_data.values.reshape(-1, 1))\n",
    "    \n",
    "#     train_x,train_y=create_train_sequences(train_data_scaled.reshape(-1),sequence_length,horizon)       \n",
    "#     train_seq_x[i,:,:]=train_x\n",
    "#     train_seq_y[i,:,:]=train_y\n",
    "       \n",
    "#     valid_x,valid_y=create_train_sequences(validation_data_scaled.reshape(-1),sequence_length,horizon)\n",
    "#     valid_seq_x[i,:,:]=valid_x\n",
    "#     valid_seq_y[i,:,:]=valid_y\n",
    "        \n",
    "#     test_x,test_y=create_test_seq(test_data_scaled.reshape(-1),sequence_length,horizon)\n",
    "#     test_seq_x[i,:,:]=test_x\n",
    "#     test_seq_y[i,:,:]=test_y\n",
    "\n",
    "#     if with_knowledge==True:\n",
    "#         know_scaler=StandardScaler()  \n",
    "#         train_know_scaled=know_scaler.fit_transform(train_know_data.values.reshape(-1,1))\n",
    "#         know_scalers.append(know_scaler)\n",
    "            \n",
    "#         validation_know_scaled=know_scaler.transform(validation_know_data.values.reshape(-1,1)) \n",
    "#         test_know_scaled=know_scaler.transform(test_know_data.values.reshape(-1,1))\n",
    "        \n",
    "#         train_know_seq_y[i,:,:]=create_train_knowledge_seq(train_know_scaled.reshape(-1),sequence_length,horizon) \n",
    "#         valid_know_seq_y[i,:,:]=create_train_knowledge_seq(validation_know_scaled.reshape(-1),sequence_length,horizon)\n",
    "#         test_know_seq_y[i,:,:]=create_test_k_seq(test_know_scaled.reshape(-1),horizon)\n",
    "\n",
    "    if attention==\"auto_correlation\" :         \n",
    "       train_attention_x[i,:,:]=create_attention_seq(train_data.reshape(-1),sequence_length, horizon,context_window)\n",
    "       valid_attention_x[i,:,:]=create_attention_seq(validation_data.reshape(-1),sequence_length, horizon,context_window)\n",
    "       test_attention_x[i,:,:]=create_attention_test_sequences(test_data,sequence_length, horizon,context_window)\n",
    "\n",
    "X_train=torch.tensor(train_seq_x,dtype=torch.float32)\n",
    "y_train=torch.tensor(train_seq_y,dtype=torch.float32)\n",
    "X_valid=torch.tensor(valid_seq_x,dtype=torch.float32)\n",
    "y_valid=torch.tensor(valid_seq_y,dtype=torch.float32)\n",
    "X_test=torch.tensor(test_seq_x,dtype=torch.float32)\n",
    "y_test=torch.tensor(test_seq_y,dtype=torch.float32)\n",
    "\n",
    "\n",
    "X_train=X_train.unsqueeze(-1)\n",
    "X_train=X_train.view(-1,X_train.size(2),X_train.size(3))\n",
    "\n",
    "y_train=y_train.unsqueeze(-1)\n",
    "y_train=y_train.view(-1,y_train.size(2),y_train.size(3))\n",
    "\n",
    "X_valid=X_valid.unsqueeze(-1)\n",
    "X_valid=X_valid.view(-1,X_valid.size(2),X_valid.size(3))\n",
    "\n",
    "y_valid=y_valid.unsqueeze(-1)\n",
    "y_valid=y_valid.view(-1,y_valid.size(2),y_valid.size(3))\n",
    "\n",
    "X_test=X_test.unsqueeze(-1)\n",
    "y_test=y_test.unsqueeze(-1)\n",
    "\n",
    "if with_knowledge==True:\n",
    "    y_know_train_seq=torch.tensor(train_know_seq_y,dtype=torch.float32)\n",
    "    y_know_valid_seq=torch.tensor(valid_know_seq_y,dtype=torch.float32)\n",
    "    y_know_test_seq=torch.tensor(test_know_seq_y,dtype=torch.float32)\n",
    "    \n",
    "    y_know_train_seq=y_know_train_seq.unsqueeze(-1)\n",
    "    Y_know_train_seq=y_know_train_seq.view(-1,y_know_train_seq.size(2),y_know_train_seq.size(3))\n",
    "\n",
    "    y_know_valid_seq=y_know_valid_seq.unsqueeze(-1)\n",
    "    Y_know_valid_seq=y_know_valid_seq.view(-1,y_know_train_seq.size(2),y_know_train_seq.size(3))\n",
    "\n",
    "    Y_know_test_seq=y_know_test_seq.unsqueeze(-1)\n",
    "\n",
    "if attention==\"auto_correlation\":\n",
    "       X_train_attention=torch.tensor(train_attention_x,dtype=torch.float32)\n",
    "       X_valid_attention=torch.tensor(valid_attention_x,dtype=torch.float32)\n",
    "       X_test_attention=torch.tensor(test_attention_x,dtype=torch.float32)\n",
    "\n",
    "       X_train_attention=X_train_attention.unsqueeze(-1)\n",
    "       X_train_attention=X_train_attention.view(-1,X_train_attention.size(-2),X_train_attention.size(3))\n",
    "\n",
    "       X_valid_attention=X_valid_attention.unsqueeze(-1)\n",
    "       X_valid_attention=X_valid_attention.view(-1,X_valid_attention.size(-2),X_valid_attention.size(3))\n",
    "\n",
    "       X_test_attention=X_test_attention.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesDataset(Dataset):\n",
    "  def __init__(self,X,y):\n",
    "    self.X=X\n",
    "    self.y=y\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "  def __getitem__(self,idx):\n",
    "    return self.X[idx],self.y[idx]\n",
    "class AttentionDataset(Dataset):\n",
    "  def __init__(self,X):\n",
    "    self.X=X\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "  def __getitem__(self,idx):\n",
    "    return self.X[idx]\n",
    "class KnowledgeDataset(Dataset):\n",
    "  def __init__(self,X):\n",
    "    self.X=X\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "  def __getitem__(self,idx):\n",
    "    return self.X[idx]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=TimeseriesDataset(X_train,y_train)\n",
    "valid_dataset=TimeseriesDataset(X_valid,y_valid)\n",
    "test_dataset=TimeseriesDataset(X_test,y_test)\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batch_size,drop_last=True)\n",
    "valid_loader=DataLoader(valid_dataset,batch_size,drop_last=True)\n",
    "test_loader=DataLoader(test_dataset,batch_size,drop_last=True)\n",
    "\n",
    "if with_knowledge==True:\n",
    "    train_know_dataset=KnowledgeDataset(Y_know_train_seq)\n",
    "    valid_know_dataset=KnowledgeDataset(Y_know_valid_seq)\n",
    "    test_know_dataset=KnowledgeDataset(Y_know_test_seq)\n",
    "\n",
    "    train_knowledge_loader=DataLoader(train_know_dataset,batch_size,drop_last=True)\n",
    "    valid_knowledge_loader=DataLoader(valid_know_dataset,batch_size,drop_last=True)\n",
    "    test_knowledge_loader=DataLoader(test_know_dataset,batch_size,drop_last=True)\n",
    "        \n",
    "if attention==\"auto_correlation\":\n",
    "   train_attention_dataset=AttentionDataset(X_train_attention)\n",
    "   valid_attention_dataset=AttentionDataset(X_valid_attention)\n",
    "   test_attention_dataset=AttentionDataset(X_test_attention)\n",
    "   \n",
    "   train_attention_loader=DataLoader(train_attention_dataset,batch_size,drop_last=True)\n",
    "   valid_attention_loader=DataLoader(valid_attention_dataset,batch_size,drop_last=True)\n",
    "   test_attention_loader=DataLoader(test_attention_dataset,batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "  def __init__(self,input_size,hidden_size):\n",
    "      super().__init__()\n",
    "      self.input_size=input_size\n",
    "      self.hidden_size=hidden_size\n",
    "      self.conv1d=nn.Conv1d(in_channels=self.input_size,out_channels=self.hidden_size,padding=1,kernel_size=3,bias=False)\n",
    "  def forward(self,x):\n",
    "      embedded_inp=self.conv1d(x.permute(0,2,1))\n",
    "      return embedded_inp.transpose(1,2)\n",
    "\n",
    "class RelativePositionalEmbedding(nn.Module):\n",
    "    def __init__(self,head_dim,max_position=512):\n",
    "        super(RelativePositionalEmbedding,self).__init__()\n",
    "        self.pos_embed=nn.Parameter(torch.Tensor(max_position * 2 + 1, head_dim))\n",
    "        nn.init.xavier_uniform_(self.pos_embed)\n",
    "        self.max_position=max_position\n",
    "    def forward(self,query_len,key_len):\n",
    "        query_range=torch.arange(query_len)\n",
    "        key_range=torch.arange(key_len)\n",
    "        relative_matrix=key_range[None,:]-query_range[:,None]\n",
    "        clipped_relative_matrix = torch.clamp(relative_matrix, -self.max_position, self.max_position)\n",
    "        relative_matrix=clipped_relative_matrix+self.max_position \n",
    "        return self.pos_embed[relative_matrix]\n",
    "    \n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()* -(math.log(10000.0) / d_model)).exp()\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self,x):\n",
    "        return self.pe[:,:x.size(1)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "  def __init__(self,attn_head,hidden_size):\n",
    "    super().__init__()\n",
    "\n",
    "    self.attn_head=attn_head\n",
    "    self.hidden_size=hidden_size\n",
    "    self.dropout = nn.Dropout(0.01)\n",
    "    self.linear=nn.Linear(hidden_size,hidden_size)\n",
    "\n",
    "    self.queries=nn.Linear(hidden_size,hidden_size)\n",
    "    self.keys=nn.Linear(hidden_size,hidden_size)\n",
    "    self.values=nn.Linear(hidden_size,hidden_size)\n",
    "\n",
    "  def forward(self, queries,keys,values,attention_mask,return_attention=False):\n",
    "    b,l,d=queries.shape\n",
    "    b,s,d=keys.shape\n",
    "    \n",
    "    #Linear projection and creation of multiple heads\n",
    "    queries=self.queries(queries).view(b,l,self.attn_head,-1)\n",
    "    keys=self.keys(keys).view(b,s,self.attn_head,-1)\n",
    "    values=self.values(values).view(b,s,self.attn_head,-1)\n",
    "\n",
    "    b,l,h,d=queries.shape\n",
    "    b,s,h,d=values.shape\n",
    "\n",
    "    #Calculate attention score\n",
    "    attention_score=torch.einsum(\"blhd,bshd->bhls\",queries,keys)\n",
    "    \n",
    "    if attention_mask == True:\n",
    "       mask_shape = [b,1,l,l]\n",
    "       mask=torch.triu(torch.ones(mask_shape,dtype=torch.bool),diagonal=1)\n",
    "       attention_score.masked_fill_(mask.to(device),-np.inf)\n",
    "    attention_score_softmax=self.dropout(torch.softmax(attention_score/sqrt(d),dim=-1))    \n",
    "    final_value=torch.einsum(\"bhls,bshd->blhd\",attention_score_softmax,values)\n",
    "    weighted_attn_val=self.linear(final_value.contiguous().view(b,l,-1))\n",
    "\n",
    "    return weighted_attn_val\n",
    "  \n",
    "class RelativeAttention(nn.Module):\n",
    "    def __init__(self, attn_head,hidden_size):\n",
    "        super(RelativeAttention, self).__init__()\n",
    "        self.attn_head=attn_head\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.01)\n",
    "        self.queries=nn.Linear(hidden_size,hidden_size)\n",
    "        self.keys=nn.Linear(hidden_size,hidden_size)\n",
    "        self.values=nn.Linear(hidden_size,hidden_size)\n",
    "        \n",
    "        self.relative_positional_emd=RelativePositionalEmbedding(int(hidden_size/self.attn_head))\n",
    "        self.linear=nn.Linear(hidden_size,hidden_size)\n",
    "    def forward(self, query, key, value,attention_mask,return_attention=False):\n",
    "        b,l,d=query.shape\n",
    "        b,s,d=key.shape\n",
    "        queries=self.queries(query).view(b,l,self.attn_head,-1)\n",
    "        keys=self.keys(key).view(b,s,self.attn_head,-1)\n",
    "        values=self.values(value).view(b,s,self.attn_head,-1)\n",
    "        \n",
    "        b,l,h,d=queries.shape\n",
    "        b,s,h,d=values.shape\n",
    "        \n",
    "        a_key=self.relative_positional_emd(l,s)\n",
    "        a_val=self.relative_positional_emd(l,s)\n",
    "        \n",
    "        qk_attention=torch.einsum(\"blhd,bshd->bhls\",queries,keys)\n",
    "        qk_relative_attention=torch.einsum(f\"blhd,lsd->bhls\",queries,a_key)\n",
    "        \n",
    "        attention_score=qk_attention+qk_relative_attention\n",
    "        \n",
    "        if attention_mask == True:\n",
    "            #print(f\"attn_score:{attn_score.shape}\")\n",
    "            mask_shape = [b,1,l,l]\n",
    "            mask=torch.triu(torch.ones(mask_shape,dtype=None),diagonal=1)\n",
    "            attention_score.masked_fill_(mask.to(device),-np.inf)\n",
    "        attention_score_softmax=self.dropout(torch.softmax(attention_score/sqrt(d),dim=-1))\n",
    "        \n",
    "        weighted_attention=torch.einsum(\"bhls,bshd->blhd\",attention_score_softmax,values)\n",
    "        weighted_attention_rel=torch.einsum(\"bhls,lsd->blhd\",attention_score_softmax,a_val)\n",
    "        \n",
    "        weighted_attention_final=weighted_attention+weighted_attention_rel\n",
    "        weighted_attn_val=self.linear(weighted_attention_final.contiguous().view(b,l,-1))\n",
    "        \n",
    "\n",
    "        return weighted_attn_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrelationAttentionLayer(nn.Module):\n",
    "  def __init__(self,attn_head,hidden_size): \n",
    "    super(CorrelationAttentionLayer,self).__init__()\n",
    "    self.attn_head=attn_head\n",
    "    self.hidden_size=hidden_size\n",
    "    self.dropout=nn.Dropout(0.1)\n",
    "\n",
    "    self.queries_emb=nn.Linear(hidden_size,hidden_size)\n",
    "    self.keys_emb=nn.Linear(hidden_size,hidden_size)\n",
    "    self.values_emb=nn.Linear(hidden_size,hidden_size)\n",
    "\n",
    "  def forward(self,q,k,v,attention_mask):\n",
    "    b,l,d=q.shape\n",
    "    b,s,d=k.shape\n",
    "    \n",
    "    #Linear projection and creation of multiple heads\n",
    "    queries=self.queries_emb(q).view(b,l,self.attn_head,-1)\n",
    "    keys=self.keys_emb(k).view(b,s,self.attn_head,-1)\n",
    "    values=self.values_emb(v).view(b,s,self.attn_head,-1)\n",
    "    \n",
    "    b,l,h,d=queries.shape\n",
    "    b,s,h,d=values.shape\n",
    "    \n",
    "    if l > s:\n",
    "            zeros = torch.zeros_like(queries[:, :(l - s), :]).float()\n",
    "            values = torch.cat([values, zeros], dim=1)\n",
    "            keys = torch.cat([keys, zeros], dim=1)\n",
    "    else:\n",
    "            values = values[:, :l, :, :]\n",
    "            keys = keys[:, :l, :, :]\n",
    "\n",
    "    top_k=5\n",
    "    \n",
    "    q_fft = torch.fft.rfft(queries.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "    k_fft = torch.fft.rfft(keys.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "    \n",
    "    res = q_fft * torch.conj(k_fft)\n",
    "    \n",
    "    corr = torch.fft.irfft(res, n=l, dim=-1)\n",
    "    \n",
    "    mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)\n",
    "    top2_values, top2_indices = torch.topk(mean_value, 2, dim=1)\n",
    "    #print(f\"top2_indices:{top2_indices}\")\n",
    "    #max_value, max_index = torch.max(mean_value, dim=1)\n",
    "    #print(f\"max_index: {max_index}\")\n",
    "    \n",
    "    index = torch.topk(torch.mean(mean_value, dim=0), top_k, dim=-1)[1]\n",
    "    weights = torch.stack([mean_value[:, index[i]] for i in range(top_k)], dim=-1)\n",
    "\n",
    "    tmp_corr = torch.softmax(weights, dim=-1)\n",
    "\n",
    "    values=values.permute(0, 2, 3, 1).contiguous()\n",
    "\n",
    "    tmp_values = values\n",
    "    delays_agg = torch.zeros_like(values).float()\n",
    "\n",
    "    for i in range(top_k):\n",
    "        pattern = torch.roll(tmp_values, -int(index[i]), -1)\n",
    "        delays_agg = delays_agg + pattern * \\\n",
    "                         (tmp_corr[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, h, d, l))\n",
    "    agg_seq=delays_agg.permute(0, 3, 1, 2).view(b,l,-1)\n",
    "\n",
    "    return agg_seq\n",
    "    \"\"\"\n",
    "    if attention==\"autoformer_context_window\" and with_knowledge==True:       \n",
    "        return agg_seq[:,:sequence_length+horizon+label_length+horizon,:]\n",
    "        \n",
    "    elif attention==\"autoformer_context_window\" and with_knowledge==False:\n",
    "        return agg_seq[:,:label_length+horizon,:]\n",
    "        \n",
    "    else:\n",
    "        return agg_seq\n",
    "    \"\"\"\n",
    "\"\"\"\n",
    "def autocorrelation(seq_win,con_win):\n",
    "    #print(f\"con_win type: {type(con_win)}\")\n",
    "    corr=np.correlate(con_win[i].cpu().numpy().reshape(-1),seq_win[i].cpu().numpy().reshape(-1),mode=\"valid\")    \n",
    "    return torch.tensor(corr,device=device)\n",
    "\"\"\"\n",
    "def get_correlation_seq(seq_win,con_win):   \n",
    "    batch_size=seq_win.shape[0]\n",
    "    #correlation_batch_index=np.zeros(batch_size)\n",
    "    #correlation_batch_val=np.zeros(batch_size)\n",
    "    correlation_seq=torch.zeros((batch_size,sequence_length+horizon,1))\n",
    "    for i in range(batch_size):\n",
    "        seq_win_batch=seq_win[i] \n",
    "        context_win_batch=con_win[i]\n",
    "        \n",
    "        context_win=context_win_batch[:-sequence_length,:]  \n",
    "        correlation=[] \n",
    "        for j in range(context_win.shape[0]-sequence_length+1):\n",
    "            sequence_window=seq_win_batch \n",
    "            #context_window_data=context_win_batch[j:j+sequence_length,:]         \n",
    "            #corr=autocorrelation(sequence_window,context_window_data) \n",
    "            context_window_data=context_win[j:j+sequence_length,:] \n",
    "            corr=dtw(sequence_window.cpu().numpy().reshape(-1),context_window_data.cpu().numpy().reshape(-1))\n",
    "            correlation.append(corr) \n",
    "                \n",
    "        min_correlation_index=torch.argmin(torch.tensor(correlation))\n",
    "        \n",
    "        min_correlation_index = min_correlation_index.clone().detach().to(device)\n",
    "        #print(f\"con_win shape:{con_win}\") \n",
    "        correlation_seq[i]=con_win[i,min_correlation_index:min_correlation_index+sequence_length+horizon,:]\n",
    "        \n",
    "        l_a,_=con_win[i,min_correlation_index:min_correlation_index+sequence_length+horizon,:].shape\n",
    "        if l_a < sequence_length+horizon:  \n",
    "            prev_index=(sequence_length+horizon)-l_a   \n",
    "            correlation_seq[i]=con_win[i,min_correlation_index-prev_index:,:]\n",
    "        else:\n",
    "            correlation_seq[i]=con_win[i,min_correlation_index:min_correlation_index+sequence_length+horizon,:]       \n",
    "    return correlation_seq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self,attention,hidden_size,output_size):\n",
    "    super(Encoder,self).__init__()\n",
    "    self.attention=attention  \n",
    "    self.conv1=nn.Conv1d(in_channels=hidden_size,out_channels=hidden_size,kernel_size=1)\n",
    "    self.conv2=nn.Conv1d(in_channels=hidden_size,out_channels=hidden_size,kernel_size=1)\n",
    "    self.linear=nn.Linear(hidden_size,hidden_size)\n",
    "    #self.norm1=nn.LayerNorm(normalized_shape=(sequence_length,hidden_size))\n",
    "    self.norm1=nn.LayerNorm(hidden_size)\n",
    "    self.activation=F.relu\n",
    "    self.dropout=nn.Dropout(0.01)\n",
    "    \n",
    "    self.queries=nn.Linear(hidden_size,hidden_size)\n",
    "    self.keys=nn.Linear(hidden_size,hidden_size)\n",
    "    self.values=nn.Linear(hidden_size,hidden_size)\n",
    "    \"\"\"\n",
    "    #For autoformer with context_window, the attention returned will be downsampled to match the size\n",
    "    #of input sequence so that it can be added as a residual connection  :\n",
    "    if with_knowledge==True:\n",
    "        self.downsample_layer=nn.Linear(context_window,sequence_length+horizon)\n",
    "    else:\n",
    "        self.downsample_layer=nn.Linear(context_window,sequence_length)\n",
    "    \"\"\"\n",
    "  def forward(self,x,x_attn=None):\n",
    "        \n",
    "    if test_with_attention==False:\n",
    "        #Skipping attention and passed through FFN only\n",
    "        out=self.conv1(x.permute(0,2,1))\n",
    "        out=self.activation(out)\n",
    "        out=self.dropout(self.conv2(out).transpose(-1, 1))\n",
    "        norm_out=self.norm1(out)\n",
    "        return norm_out \n",
    "    else:\n",
    "        if attention==\"auto_correlation\": \n",
    "            #pass the most correlated sequence_window + the horizon following it\n",
    "            #to get the attention \n",
    "            attention_x=self.attention(x_attn,x_attn,x_attn,attention_mask=False)\n",
    "            #skipped residual connectoin here, otherwise to match the sizes of attention with context\n",
    "            # and original input (window_size), we had to downsample attention(previous_datapoints+window_size) output .\n",
    "            new_x=attention_x \n",
    "        else:\n",
    "            attention_x=self.attention(x,x,x,attention_mask=False) \n",
    "            new_x = x + attention_x      \n",
    "        res_x=x=self.norm1(new_x)\n",
    "        ##Feed forward NN: \n",
    "        out=self.conv1(res_x.permute(0,2,1))\n",
    "        out=self.dropout(self.activation(out)) \n",
    "        out=self.dropout(self.conv2(out).transpose(-1, 1))\n",
    "\n",
    "        ##Add and normalize:\n",
    "        new_out=out+res_x\n",
    "        norm_out=self.norm1(new_out)\n",
    "        \n",
    "        return norm_out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self,attention,hidden_size,output_size):\n",
    "    super(Decoder,self).__init__()\n",
    "    self.attention=attention\n",
    "    self.conv1=nn.Conv1d(in_channels=hidden_size,out_channels=hidden_size,kernel_size=1)\n",
    "    self.conv2=nn.Conv1d(in_channels=hidden_size,out_channels=hidden_size,kernel_size=1)\n",
    "\n",
    "    self.linear=nn.Linear(hidden_size,output_size)\n",
    "    self.dropout=nn.Dropout(0.01)\n",
    "    self.norm1=nn.LayerNorm(hidden_size)\n",
    "    self.norm2=nn.LayerNorm(hidden_size)\n",
    "    self.norm3=nn.LayerNorm(hidden_size)\n",
    "    self.activation=F.relu\n",
    "\n",
    "  def forward(self,dec_inp,enc_out):\n",
    "    \n",
    "    if test_with_attention==False:\n",
    "        \n",
    "        #FFN\n",
    "        out=self.conv1(enc_out.permute(0,2,1))\n",
    "        out=self.activation(out)\n",
    "        out=self.dropout(self.conv2(out).transpose(-1, 1))\n",
    "        out=self.norm3(out)\n",
    "        \n",
    "        #Linear projection\n",
    "        pred=self.linear(out)\n",
    "        return pred\n",
    "    \n",
    "    else:  \n",
    "        #print(f\"in decoder\")\n",
    "        self_attn=self.attention(dec_inp,dec_inp,dec_inp,attention_mask=True)\n",
    "        #add residual connection and normalize\n",
    "        \n",
    "        residual_add=dec_inp + self_attn\n",
    "        new_dec_x=self.norm1(residual_add)\n",
    "    \n",
    "        # encoder-decoder attention. Pass encoder output as key and value and queries as output of self-attention of decoder\n",
    "        enc_dec_atten=self.attention(new_dec_x,enc_out,enc_out,attention_mask=False)\n",
    "        \n",
    "        ## add and normalize\n",
    "        new_x=enc_dec_atten+self_attn\n",
    "        norm_x=self.norm2(new_x)\n",
    "    \n",
    "        #FFN\n",
    "        out=self.conv1(norm_x.permute(0,2,1))\n",
    "        out=self.dropout(self.activation(out))\n",
    "        out=self.dropout(self.conv2(out).transpose(-1, 1))\n",
    "\n",
    "        #add and normalize\n",
    "        new_x=out+norm_x\n",
    "        out=self.norm3(new_x)\n",
    "\n",
    "        #Linear projection\n",
    "        pred=self.linear(out)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "  def __init__(self,input_size,hidden_size,output_size,attn_head):\n",
    "      super(TransformerModel,self).__init__()\n",
    "      \n",
    "      self.enc_embedding=InputEmbedding(input_size,hidden_size)\n",
    "      self.dec_embedding=InputEmbedding(input_size,hidden_size)\n",
    "        \n",
    "      if positional_embedding==\"absolute\":  \n",
    "            self.enc_positional_embedding=PositionalEmbedding(hidden_size)\n",
    "            self.dec_positional_embedding=PositionalEmbedding(hidden_size)\n",
    "            if attention==\"vanilla\" or attention==\"auto_correlation\":\n",
    "                    self.encoder=Encoder( AttentionLayer(attn_head,hidden_size),hidden_size,output_size)\n",
    "                    self.decoder=Decoder( AttentionLayer(attn_head,hidden_size),hidden_size,output_size)\n",
    "            elif attention==\"autoformer\": \n",
    "                    self.encoder=Encoder( CorrelationAttentionLayer(attn_head,hidden_size),hidden_size,output_size)  \n",
    "                    self.decoder=Decoder( CorrelationAttentionLayer(attn_head,hidden_size),hidden_size,output_size)\n",
    "            \n",
    "      else:    \n",
    "            #Uses Relative positional embedding, used during vanilla self attention\n",
    "            self.encoder=Encoder( RelativeAttention(attn_head,hidden_size),hidden_size,output_size)\n",
    "            self.decoder=Decoder( RelativeAttention(attn_head,hidden_size),hidden_size,output_size)\n",
    "            \n",
    "            \n",
    "            #self.encoders = nn.ModuleList([Encoder(AttentionLayer(self.attn_head, hidden_size), hidden_size, output_size, ff_hiddensize, sequence_length)\n",
    "                                        #for _ in range(2)])\n",
    "            #self.decoders = nn.ModuleList([Decoder( AttentionLayer(self.attn_head,hidden_size),self.hidden_size,self.output_size,self.ff_hidden_size,sequence_length )\n",
    "                                        #for _ in range(2)])\n",
    "                \n",
    "      self.linear=nn.Linear(hidden_size,output_size)\n",
    "      self.dropout=nn.Dropout(0.01) \n",
    "      \n",
    "  def forward(self,x,y,x_attn=None,know_pred=None):\n",
    "        #print(f\"in model forward\")\n",
    "        if know_pred is not None:  \n",
    "            #1.For encoder, Integrate knowledge predictions to the sequence_window(x) to get the context vector at the encoder that has- \n",
    "            #-informataion of future using the knowledge predictions\n",
    "            #2.For decoder, when knowledge pred were not used, just the previous four data points from the forecast horizon x[:,-4:,:]\n",
    "            #was included. Now, knowledge predictions are also integrated here.\n",
    "            \n",
    "            encoder_input=torch.cat((x,know_pred),dim=1)\n",
    "            decoder_input=torch.cat((x[:,-4:,:],torch.zeros_like(y[:,-horizon:,:])),dim=1)\n",
    "        else:\n",
    "            encoder_input=x\n",
    "            decoder_input=torch.cat((x[:,-4:,:],torch.zeros_like(y[:,-horizon:,:])),dim=1)\n",
    "      \n",
    "        #ENCODER\n",
    "        \n",
    "        #decide here which positional embedding\n",
    "        if positional_embedding==\"absolute\":\n",
    "            inp_embed=self.enc_embedding(encoder_input)\n",
    "            pos_embed=self.enc_positional_embedding(encoder_input)\n",
    "            enc_out=inp_embed + pos_embed\n",
    "            \n",
    "            inp_embed=self.dec_embedding(decoder_input)\n",
    "            pos_embed=self.dec_positional_embedding(decoder_input)\n",
    "            dec_out= inp_embed + pos_embed\n",
    "            \"\"\"\n",
    "            if attention==\"auto_correlation\":\n",
    "                inp_embed=self.enc_embedding(x_attn)\n",
    "                pos_embed=self.enc_positional_embedding(x_attn)\n",
    "                enc_out_attn_context=inp_embed + pos_embed\n",
    "            \"\"\"\n",
    "        else:  \n",
    "            # In relative positional embedding, the positional embedding are learnt on the fly by the model during attention. Used Shaw et al., method,\n",
    "            # hence, there is only input embedding,positional embedding part in attention.\n",
    "            enc_out=self.enc_embedding(encoder_input)\n",
    "            dec_out=self.dec_embedding(decoder_input)\n",
    "        \n",
    "        # After input embedding check which attention must be used.\n",
    "        if attention==\"auto_correlation\":\n",
    "            #get the most correlated sequence+horizon of \"X\" in \"X_attn(Context_window)\"\n",
    "            corr_seq=get_correlation_seq(x,x_attn) \n",
    "            \n",
    "            attn_inp_embed=self.enc_embedding(corr_seq)\n",
    "            attn_pos_embed=self.enc_positional_embedding(corr_seq)\n",
    "            \n",
    "            attn_enc_out=attn_inp_embed+attn_pos_embed  \n",
    "            enc_out=self.encoder(enc_out,attn_enc_out)        \n",
    "        else:\n",
    "          # if attention is vanilla attention/autoformer attention:\n",
    "            enc_out=self.encoder(enc_out)           \n",
    "        #decoder      \n",
    "        out=self.decoder(dec_out,enc_out)        \n",
    "        return out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 1: expected a matrix at /usr/src/packages/BUILD/pytorch-1.0rc1/aten/src/TH/generic/THTensorMoreMath.cpp:1270",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-5014cd57e0bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m               \u001b[0;31m# Simple vanilla transformer with knowledge prediction integrated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m               \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknow_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_knowledge_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                     \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mknow_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                     \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-92-4f19af60eb64>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, x_attn, know_pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0menc_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m#decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menc_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-9d8771f33c0a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, dec_inp, enc_out)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m#print(f\"in decoder\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;31m#add residual connection and normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-dd61c95c72ca>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, attention_mask, return_attention)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m#print(f\"attn_score:{attn_score.shape}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mmask_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mattention_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mattention_score_softmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_score\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 1: expected a matrix at /usr/src/packages/BUILD/pytorch-1.0rc1/aten/src/TH/generic/THTensorMoreMath.cpp:1270"
     ]
    }
   ],
   "source": [
    "model=TransformerModel(input_size,hidden_size,output_size,attn_head)\n",
    "\n",
    "model=model.to(device)\n",
    "loss_fun=nn.MSELoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "epochs=5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        train_loss=[]\n",
    "        valid_total_loss=[]\n",
    "        model.train()\n",
    "        \n",
    "        if with_knowledge==False and (attention==\"auto_correlation\"):\n",
    "              \n",
    "              for (batch_idx, (X,y)),(batch_idx_3,(x_attention)) in zip(enumerate(train_loader), enumerate(train_attention_loader)):\n",
    "                    \n",
    "                    #1.Pass the window,forecast horizon,knowledge prediction and a context window of say 120 time points-\n",
    "                    #-that are present upto the end of current window.\n",
    "                    #2.Get the most correlated sequence index and the data points of window_size after the correlated index (np.correlate (context_window,sequence_window)) \n",
    "                    #3.Pass this correlated sequence as keys and values to the attention and sequence_window as query in attention mechanism\n",
    "                    \n",
    "                    pred=model(X.to(device),y.to(device),x_attention.to(device))\n",
    "                    pred=pred[:,-horizon:,:]\n",
    "                    optimizer.zero_grad()\n",
    "                    loss=loss_fun(pred,y.to(device))\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    train_loss.append(loss.item())\n",
    "              train_loss = np.average(train_loss)      \n",
    "        \n",
    "              model.eval()\n",
    "              with torch.no_grad():\n",
    "               for (batch_idx, (X,y)),(batch_idx_3,(x_attention)) in zip(enumerate(valid_loader), enumerate(valid_attention_loader)):\n",
    "                    pred=model(X.to(device),y.to(device),x_attention.to(device))\n",
    "                    pred=pred[:,-horizon:,:]\n",
    "            \n",
    "                    valid_loss=loss_fun(pred,y.to(device)) \n",
    "                    valid_total_loss.append(valid_loss.item())\n",
    "               valid_total_loss=np.average(valid_total_loss)\n",
    "              print(f\"epoch:{epoch} train_loss:{train_loss} valid_loss:{valid_total_loss}\") \n",
    "\n",
    "        elif with_knowledge == True and (attention ==\"vanilla\" or attention==\"autoformer\"):  \n",
    "              attn=None\n",
    "              # Simple vanilla transformer with knowledge prediction integrated\n",
    "              for (batch_idx, (X,y)),(batch_idx_2,(know_pred)) in zip(enumerate(train_loader), enumerate(train_knowledge_loader)): \n",
    "                    pred=model(X.to(device),y.to(device),attn,know_pred.to(device))\n",
    "                    pred=pred[:,-horizon:,:]\n",
    "                    optimizer.zero_grad()\n",
    "                    loss=loss_fun(pred,y.to(device))\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    train_loss.append(loss.item())\n",
    "            \n",
    "              train_loss = np.average(train_loss)\n",
    "        \n",
    "              model.eval()\n",
    "        \n",
    "              with torch.no_grad():\n",
    "          \n",
    "               for (batch_idx, (X,y)),(batch_idx_2,(know_pred)) in zip(enumerate(valid_loader), enumerate(valid_knowledge_loader)):\n",
    "                   pred=model(X.to(device),y.to(device),attn,know_pred.to(device))\n",
    "                   pred=pred[:,-horizon:,:]\n",
    "    \n",
    "                   valid_loss=loss_fun(pred,y.to(device)) \n",
    "                   valid_total_loss.append(valid_loss.item())\n",
    "            \n",
    "               valid_total_loss=np.average(valid_total_loss)          \n",
    "              print(f\"epoch:{epoch} train_loss:{train_loss} valid_loss:{valid_total_loss}\") \n",
    "\n",
    "        elif with_knowledge == False and (attention ==\"vanilla\" or attention==\"autoformer\"): \n",
    "\n",
    "            for (batch_idx, (X,y)) in enumerate(train_loader): \n",
    "                    pred=model(X.to(device),y.to(device))\n",
    "                    pred=pred[:,-horizon:,:]\n",
    "                    optimizer.zero_grad()\n",
    "                    loss=loss_fun(pred,y.to(device))\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                     \n",
    "                    train_loss.append(loss.item())\n",
    "            \n",
    "            train_loss = np.average(train_loss)\n",
    "        \n",
    "            model.eval()\n",
    "        \n",
    "            with torch.no_grad():\n",
    "          \n",
    "                for (batch_idx, (X,y)) in enumerate(valid_loader):\n",
    "                   pred=model(X.to(device),y.to(device))\n",
    "                   pred=pred[:,-horizon:,:]\n",
    "                   \n",
    "                   valid_loss=loss_fun(pred,y.to(device)) \n",
    "                   valid_total_loss.append(valid_loss.item())\n",
    "            \n",
    "                valid_total_loss=np.average(valid_total_loss)      \n",
    "            print(f\"epoch:{epoch} train_loss:{train_loss} valid_loss:{valid_total_loss}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 1: expected a matrix at /usr/src/packages/BUILD/pytorch-1.0rc1/aten/src/TH/generic/THTensorMoreMath.cpp:1270",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-56046827c35a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mcurrent_y_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_X_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_y_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_know_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-92-4f19af60eb64>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, x_attn, know_pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0menc_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m#decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menc_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-9d8771f33c0a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, dec_inp, enc_out)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m#print(f\"in decoder\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;31m#add residual connection and normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-dd61c95c72ca>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, attention_mask, return_attention)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m#print(f\"attn_score:{attn_score.shape}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mmask_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mattention_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mattention_score_softmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_score\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 1: expected a matrix at /usr/src/packages/BUILD/pytorch-1.0rc1/aten/src/TH/generic/THTensorMoreMath.cpp:1270"
     ]
    }
   ],
   "source": [
    "output=[]\n",
    "pred_series=[]\n",
    "truth_series=[]\n",
    "loss=[]\n",
    "pred_total=[]\n",
    "y_total=[]\n",
    "enc_attention_map=[]\n",
    "dec_self_attention_map=[]\n",
    "dec_cross_attention_map=[]\n",
    "\n",
    "\n",
    "for i in range(X_test.size(0)):\n",
    "    if with_knowledge==False and (attention==\"auto_correlation\"):\n",
    "            \n",
    "            current_X_test=X_test[i,:,:,:]  \n",
    "            current_y_test=y_test[i,:,:,:]\n",
    "            current_X_attention=X_test_attention[i,:,:,:]\n",
    "                        \n",
    "            pred=model(current_X_test.to(device),current_y_test.to(device),current_X_attention.to(device))\n",
    "            pred=pred[:,-horizon:,:]\n",
    "            pred=pred.reshape(-1,1).detach().cpu().numpy()\n",
    "            current_y_test=current_y_test.reshape(-1,1).detach().numpy()\n",
    "  \n",
    "            pred_raw=scalers[i].inverse_transform(pred)\n",
    "            current_y_test_raw=scalers[i].inverse_transform(current_y_test)\n",
    "            loss.append(loss_fun(torch.tensor(pred_raw),torch.tensor(current_y_test_raw)))\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(current_y_test_raw, label='Ground Truth')\n",
    "            plt.plot(pred_raw, label='Predicted')\n",
    "            plt.title(f'Time Series {i+1}: Ground Truth vs Predicted Values')\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Value')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            #plt.show()\n",
    "            plt.savefig(os.path.join(plot_dir, f'Time_Series_{i+1}_plot.png'))\n",
    "            plt.close()\n",
    "\n",
    "    elif with_knowledge==True and (attention==\"autoformer\" or attention==\"vanilla\"):\n",
    "            attn=None\n",
    "            current_X_test=X_test[i,:,:,:]\n",
    "            current_know_pred=Y_know_test_seq[i,:,:,:]\n",
    "            current_y_test=y_test[i,:,:,:]\n",
    "            \n",
    "            pred=model(current_X_test.to(device),current_y_test.to(device),attn,current_know_pred.to(device))\n",
    "            pred=pred[:,-horizon:,:]\n",
    "            pred=pred.reshape(-1,1).detach().cpu().numpy()\n",
    "            current_y_test=current_y_test.reshape(-1,1).detach().numpy()\n",
    "  \n",
    "            pred_raw=scalers[i].inverse_transform(pred)\n",
    "            current_y_test_raw=scalers[i].inverse_transform(current_y_test)\n",
    "\n",
    "            loss.append(loss_fun(torch.tensor(pred_raw),torch.tensor(current_y_test_raw)))\n",
    "                        \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(current_y_test_raw, label='Ground Truth')\n",
    "            plt.plot(pred_raw, label='Predicted')\n",
    "            plt.title(f'Time Series {i+1}: Ground Truth vs Predicted Values')\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Value')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            #plt.show()\n",
    "            plt.savefig(os.path.join(plot_dir, f'Time_Series_{i+1}_plot.png'))\n",
    "            plt.close()\n",
    "    #Just vanilla        \n",
    "    elif with_knowledge==False:\n",
    "            \n",
    "            current_X_test=X_test[i,:,:,:]\n",
    "            current_y_test=y_test[i,:,:,:]\n",
    "            \n",
    "            pred=model(current_X_test.to(device),current_y_test.to(device))\n",
    "            pred=pred[:,-horizon:,:]\n",
    "            pred=pred.reshape(-1,1).detach().cpu().numpy()\n",
    "            current_y_test=current_y_test.reshape(-1,1).detach().numpy()\n",
    "  \n",
    "            pred_raw=scalers[i].inverse_transform(pred)\n",
    "            current_y_test_raw=scalers[i].inverse_transform(current_y_test)\n",
    "            \"\"\"\n",
    "            if(i==1):\n",
    "                print(f\"pred_raw:{pred_raw} current_y_test_raw:{current_y_test_raw}\")\n",
    "            \"\"\"\n",
    "            loss.append(loss_fun(torch.tensor(pred_raw),torch.tensor(current_y_test_raw)))\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(current_y_test_raw, label='Ground Truth')\n",
    "            plt.plot(pred_raw, label='Predicted')\n",
    "            plt.title(f'Time Series {i+1}: Ground Truth vs Predicted Values')\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Value')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            #plt.show()\n",
    "            plt.savefig(os.path.join(plot_dir, f'Time_Series_{i+1}_plot.png'))\n",
    "            plt.close()\n",
    "            \n",
    "loss_val=torch.stack(loss,dim=0)\n",
    "mean_loss=torch.mean(loss_val)\n",
    "\n",
    "print(f\"mean loss:{mean_loss}, loss_val:{loss_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0a0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
